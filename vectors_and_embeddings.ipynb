{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3ef8ba",
   "metadata": {},
   "source": [
    "Qdrant Core Data Model: The Point\n",
    "In Qdrant, the fundamental data entity is a Point. Each point is comprised of three main components: an ID, a vector, and an optional payload.\n",
    "\n",
    "Point ID:\n",
    "\n",
    "Serves as a unique identifier for each point within Qdrant.\n",
    "Used to retrieve, update, or delete vectors.\n",
    "Can be either a 64-bit positive integer or a Universally Unique Identifier (UUID). Qdrant will assign a random UUID if not explicitly provided.\n",
    "Vectors: These are crucial for representing your data in a vector space, defining how points relate to each other based on similarity.\n",
    "\n",
    "Dense Vectors:\n",
    "Most commonly generated by embedding models or neural networks.\n",
    "These models learn context and meaning from large datasets, placing similar items close together in the vector space. For example, text embedding models can capture semantic relationships between sentences or words.\n",
    "Dimensionality Considerations: The number of dimensions in a vector impacts both search quality and performance.\n",
    "Smaller dimensions (e.g., 384-512) are faster and more performant but capture less detail.\n",
    "Mid-sized dimensions (e.g., 768-1536) offer a good balance between performance and accuracy.\n",
    "Very high dimensions (e.g., over 3,000) provide high precision but can significantly impact performance and memory usage, especially at scale.\n",
    "Sparse Vectors:\n",
    "High-dimensional vectors where most values are zero.\n",
    "Typically generated by statistical methods like BM25 or sparse neural encoders.\n",
    "Especially useful for keyword search and techniques like hybrid search.\n",
    "Multi-vectors:\n",
    "Allows storing multiple vectors per point, essentially a matrix of dense vectors.\n",
    "Useful for advanced techniques like late interaction.\n",
    "Named Vectors: Qdrant supports named vectors, which allow you to store and manage multiple types of vectors (dense, sparse, multi-vectors) within a single point. Sparse vectors specifically must be named.\n",
    "Embedding Models: Generating Vectors\n",
    "Choosing the right embedding model is key to creating effective vectors. Here are three main approaches to consider:\n",
    "\n",
    "Fast Embed:\n",
    "\n",
    "An optimized, low-latency, and CPU-friendly embedding solution designed specifically for Qdrant.\n",
    "Eliminates the need for heavy frameworks, using quantized model weights and ONNX runtime.\n",
    "Maintains competitive accuracy with a small model size (e.g., default 384 dimensions).\n",
    "Ideal for on-premise embeddings, avoiding GPU dependencies, and high CPU inference, with tight integration with Qdrant.\n",
    "Hugging Face Models:\n",
    "\n",
    "Offers a vast collection of high-quality and versatile models for various data types, domains, languages, and tasks.\n",
    "Dimensionality can vary significantly depending on the model.\n",
    "Allows for fine-tuning models to specific datasets.\n",
    "A good open-source option for tailored models, especially if you need full control over the selection and fine-tuning process, or if you can leverage a GPU for larger models. Requires the sentence-transformers library.\n",
    "Cloud-based Embedding Models (e.g., OpenAI, Gina AI):\n",
    "\n",
    "Provide very high-quality embeddings through managed APIs.\n",
    "No local compute is needed, as all processing is handled by the provider's API.\n",
    "Often feature higher dimensionality and precision.\n",
    "Suitable for prioritizing ease of use, cloud scalability, multilingual support, and state-of-the-art accuracy, especially if you don't mind API costs and latency for high-quality results or have fluctuating workloads.\n",
    "The best embedding model choice ultimately depends on your specific data type and domain requirements.\n",
    "\n",
    "Payloads: Adding Contextual Metadata\n",
    "Payloads hold structured metadata associated with a point, providing extra information beyond the vector itself.\n",
    "\n",
    "Purpose: Payloads are essential for filtering and ranking search results based on criteria not directly encoded in the vector. For example, filtering image search results by date taken or specific tags.\n",
    "Types of Payloads: Qdrant supports various data types for payloads:\n",
    "Scalar: Numbers or booleans (e.g., prices, ratings).\n",
    "Categorical: Tags (e.g., product categories, brands).\n",
    "Geolocation: Latitude and longitude pairs for location-based filtering.\n",
    "Timestamps: Date and time values.\n",
    "Arrays: Multiple values.\n",
    "Nested Objects: More complex, hierarchical data structures.\n",
    "Payload Filters: You can apply filters to narrow down search results based on payload values:\n",
    "Logical Filters: and, or, not (used with must, should, and must not in queries) to combine or exclude conditions.\n",
    "Match: Finds points with an exact value for a payload field (e.g., category equals electronics).\n",
    "Match Any: Finds points where a payload field contains any of the specified values (e.g., color is red or blue).\n",
    "Match Except: Finds payloads that do not contain the given values.\n",
    "Range: Filters numerical values within a specified range (e.g., price between 50 and 200)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1290c2d8",
   "metadata": {},
   "source": [
    "Distance Metrics in Vector Search\n",
    "\n",
    "Introduction:\n",
    "\n",
    "Vector search uses distance to define similarity.\n",
    "\"Closeness\" in vector space depends on the chosen metric, each telling a different story about similarity.\n",
    "Key Metrics:\n",
    "\n",
    "Cosine Similarity\n",
    "\n",
    "Measures: Primarily focuses on direction. It calculates the cosine of the angle between vectors.\n",
    "Key Difference: Ignores vector length (magnitude). Vectors pointing in the same direction, even if one is longer, are considered highly similar.\n",
    "Useful Contexts: Ideal when semantic meaning is conveyed by direction, not magnitude. For example, text embeddings where words like \"joyful\" and \"happy\" have the same meaning despite potentially different embedding lengths.\n",
    "Avoid When: Vector length contains important information you want to capture.\n",
    "Euclidean Distance\n",
    "\n",
    "Measures: The absolute straight-line distance between two points/vectors.\n",
    "Key Difference: Captures the true physical or numerical distance across all dimensions.\n",
    "Useful Contexts: Best when every dimension matters equally and numerical distance is meaningful. Examples include image embeddings, spatial data, or sensor data where absolute values are important.\n",
    "Avoid When: Your data is not scaled or normalized, as large features can disproportionately affect the distance. Not suitable for purely semantic similarity if magnitudes are irrelevant.\n",
    "Dot Product\n",
    "\n",
    "Measures: Similarity with magnitude taken into account. It rewards vectors that are both aligned and large.\n",
    "Key Difference: Unlike Cosine Similarity, it amplifies similarity for longer vectors that point in the same direction.\n",
    "Useful Contexts: Valuable when magnitude itself is a significant part of the signal. For instance, in recommendation engines where a larger vector might signify a stronger preference or higher interaction.\n",
    "Avoid When: Vector magnitude varies unintentionally (e.g., due to different document sizes), as it will introduce unwanted weighting.\n",
    "Conclusion:\n",
    "\n",
    "There's no single \"best\" metric. The choice depends entirely on your specific data and how you define similarity for your application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11182605",
   "metadata": {},
   "source": [
    "The Importance of Chunking: Storing an entire document as a single vector often leads to less precise search results. Chunking addresses this by breaking down large, unstructured documents into smaller, semantically relevant pieces. This approach enables more specific and accurate retrieval, while also optimizing computational resources and token usage when working with large language models.\n",
    "- Benefits and Metadata: By segmenting documents into logical units like paragraphs, headings, or subsections, each chunk can be embedded individually, capturing more precise context. A significant advantage is the ability to attach metadata to each chunk. This metadata, such as section titles, page numbers, or source URLs, is invaluable for organizing and identifying the chunks, significantly improving search and filtering capabilities.\n",
    "- Diverse Chunking Strategies: The video explores several effective methods for segmenting documents:\n",
    " - - Fixed-size chunking: A straightforward method that defines a set number of tokens per chunk, often with a slight overlap to preserve context. While simple, it risks splitting content mid-sentence.\n",
    " - - Sentence-based chunking: This strategy breaks documents into individual sentences, then groups them under a specified word limit. This ensures each chunk generally represents a complete thought, though chunk lengths can vary.\n",
    " - - Paragraph-based chunking: This method leverages the natural structure of paragraphs, which typically group semantically related ideas. However, paragraph lengths are unpredictable, sometimes requiring additional token limits or fallback splitting.\n",
    " - - Sliding window chunking: This technique uses overlapping windows to maintain continuity across chunks, ensuring no context is lost. While effective for comprehensive retrieval, it can increase storage and computational costs due to redundancy.\n",
    " - - Recursive splitting: Ideal for unstructured or inconsistent data, this method employs a hierarchical approach, attempting to split on large blocks first, then progressively falling back to smaller units like sentences, words, or characters. This logic is often integrated into tools like LangChain and LlamaIndex.\n",
    " - - Semantic chunking: The most powerful strategy, this method uses embeddings to identify and break documents at points where the topic or semantic coherence shifts. Although it is the most computationally intensive and costly, it offers the highest likelihood of ensuring each chunk contains a truly coherent idea.\n",
    "\n",
    "Leveraging Metadata in Qdrant: Metadata, stored in Qdrant's payloads, is essential for effective organization and identification of chunks. It facilitates filtering by various criteria, such as article name or tags. When Qdrant retrieves relevant points, the associated payload, including the original content, section, or source link, is also returned, eliminating the need for subsequent lookups.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qdrant-deep-dive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
